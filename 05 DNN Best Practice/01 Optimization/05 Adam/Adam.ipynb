{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05 Optimization Adam.ipynb","provenance":[],"collapsed_sections":["MMafKvUV20fg"],"authorship_tag":"ABX9TyPYPjXYUIneueBVT/X+m1ia"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y2uiG7Ah2yRw","colab_type":"text"},"source":["# Optimization Methods\n","\n","Until now, you've always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"MMafKvUV20fg","colab_type":"text"},"source":["# Function"]},{"cell_type":"code","metadata":{"id":"c35i-2sU2nrK","colab_type":"code","cellView":"form","outputId":"78763dce-a34f-4fd3-f3ab-6efc73a43528","executionInfo":{"status":"ok","timestamp":1591002975450,"user_tz":-120,"elapsed":1547,"user":{"displayName":"Nicol√≤ Marana","photoUrl":"","userId":"03353917979045990121"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["#@title External Code\n","import numpy as np\n","\n","def update_parameters_with_gd_test_case():\n","    np.random.seed(1)\n","    learning_rate = 0.01\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    \n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    \n","    return parameters, grads, learning_rate\n","\n","\"\"\"\n","def update_parameters_with_sgd_checker(function, inputs, outputs):\n","    if function(inputs) == outputs:\n","        print(\"Correct\")\n","    else:\n","        print(\"Incorrect\")\n","\"\"\"\n","\n","def random_mini_batches_test_case():\n","    np.random.seed(1)\n","    mini_batch_size = 64\n","    X = np.random.randn(12288, 148)\n","    Y = np.random.randn(1, 148) < 0.5\n","    return X, Y, mini_batch_size\n","\n","def initialize_velocity_test_case():\n","    np.random.seed(1)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","def update_parameters_with_momentum_test_case():\n","    np.random.seed(1)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    v = {'dW1': np.array([[ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.]]), 'dW2': np.array([[ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.],\n","        [ 0.,  0.,  0.]]), 'db1': np.array([[ 0.],\n","        [ 0.]]), 'db2': np.array([[ 0.],\n","        [ 0.],\n","        [ 0.]])}\n","    return parameters, grads, v\n","    \n","def initialize_adam_test_case():\n","    np.random.seed(1)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","def update_parameters_with_adam_test_case():\n","    np.random.seed(1)\n","    v, s = ({'dW1': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'dW2': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'db1': np.array([[ 0.],\n","         [ 0.]]), 'db2': np.array([[ 0.],\n","         [ 0.],\n","         [ 0.]])}, {'dW1': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'dW2': np.array([[ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.],\n","         [ 0.,  0.,  0.]]), 'db1': np.array([[ 0.],\n","         [ 0.]]), 'db2': np.array([[ 0.],\n","         [ 0.],\n","         [ 0.]])})\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    \n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    \n","    return parameters, grads, v, s\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy.io\n","import sklearn\n","import sklearn.datasets\n","\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s\n","\n","def relu(x):\n","    \"\"\"\n","    Compute the relu of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- relu(x)\n","    \"\"\"\n","    s = np.maximum(0,x)\n","    \n","    return s\n","\n","def load_params_and_grads(seed=1):\n","    np.random.seed(seed)\n","    W1 = np.random.randn(2,3)\n","    b1 = np.random.randn(2,1)\n","    W2 = np.random.randn(3,3)\n","    b2 = np.random.randn(3,1)\n","\n","    dW1 = np.random.randn(2,3)\n","    db1 = np.random.randn(2,1)\n","    dW2 = np.random.randn(3,3)\n","    db2 = np.random.randn(3,1)\n","    \n","    return W1, b1, W2, b2, dW1, db1, dW2, db2\n","\n","\n","def initialize_parameters(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    b1 -- bias vector of shape (layer_dims[l], 1)\n","                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n","                    bl -- bias vector of shape (1, layer_dims[l])\n","                    \n","    Tips:\n","    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n","    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n","    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n","        assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n","        \n","    return parameters\n","\n","\n","def compute_cost(a3, Y):\n","    \n","    \"\"\"\n","    Implement the cost function\n","    \n","    Arguments:\n","    a3 -- post-activation, output of forward propagation\n","    Y -- \"true\" labels vector, same shape as a3\n","    \n","    Returns:\n","    cost - value of the cost function\n","    \"\"\"\n","    m = Y.shape[1]\n","    \n","    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n","    cost = 1./m * np.sum(logprobs)\n","    \n","    return cost\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the loss) presented in Figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape ()\n","                    b1 -- bias vector of shape ()\n","                    W2 -- weight matrix of shape ()\n","                    b2 -- bias vector of shape ()\n","                    W3 -- weight matrix of shape ()\n","                    b3 -- bias vector of shape ()\n","    \n","    Returns:\n","    loss -- the loss function (vanilla logistic loss)\n","    \"\"\"\n","    \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    z1 = np.dot(W1, X) + b1\n","    a1 = relu(z1)\n","    z2 = np.dot(W2, a1) + b2\n","    a2 = relu(z2)\n","    z3 = np.dot(W3, a2) + b3\n","    a3 = sigmoid(z3)\n","    \n","    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n","    \n","    return a3, cache\n","\n","def backward_propagation(X, Y, cache):\n","    \"\"\"\n","    Implement the backward propagation presented in figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n","    cache -- cache output from forward_propagation()\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    m = X.shape[1]\n","    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n","    \n","    dz3 = 1./m * (a3 - Y)\n","    dW3 = np.dot(dz3, a2.T)\n","    db3 = np.sum(dz3, axis=1, keepdims = True)\n","    \n","    da2 = np.dot(W3.T, dz3)\n","    dz2 = np.multiply(da2, np.int64(a2 > 0))\n","    dW2 = np.dot(dz2, a1.T)\n","    db2 = np.sum(dz2, axis=1, keepdims = True)\n","    \n","    da1 = np.dot(W2.T, dz2)\n","    dz1 = np.multiply(da1, np.int64(a1 > 0))\n","    dW1 = np.dot(dz1, X.T)\n","    db1 = np.sum(dz1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n","                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n","                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients\n","\n","def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  n-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    p = np.zeros((1,m), dtype = np.int)\n","    \n","    # Forward propagation\n","    a3, caches = forward_propagation(X, parameters)\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, a3.shape[1]):\n","        if a3[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","\n","    # print results\n","\n","    #print (\"predictions: \" + str(p[0,:]))\n","    #print (\"true labels: \" + str(y[0,:]))\n","    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n","    \n","    return p\n","\n","def load_2D_dataset():\n","    data = scipy.io.loadmat('datasets/data.mat')\n","    train_X = data['X'].T\n","    train_Y = data['y'].T\n","    test_X = data['Xval'].T\n","    test_Y = data['yval'].T\n","\n","    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    \n","    return train_X, train_Y, test_X, test_Y\n","\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n","    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Predict the function value for the whole grid\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.ylabel('x2')\n","    plt.xlabel('x1')\n","    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n","    plt.show()\n","    \n","def predict_dec(parameters, X):\n","    \"\"\"\n","    Used for plotting decision boundary.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (m, K)\n","    \n","    Returns\n","    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    # Predict using forward propagation and a classification threshold of 0.5\n","    a3, cache = forward_propagation(X, parameters)\n","    predictions = (a3 > 0.5)\n","    return predictions\n","\n","def load_dataset():\n","    np.random.seed(3)\n","    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n","    # Visualize the data\n","    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    train_X = train_X.T\n","    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n","    \n","    return train_X, train_Y\n","    "],"execution_count":1,"outputs":[{"output_type":"stream","text":["<ipython-input-1-85280c2787ba>:182: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n","  assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n","<ipython-input-1-85280c2787ba>:183: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n","  assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"86Xghvj3Dz8K","colab_type":"text"},"source":["# Import Library"]},{"cell_type":"code","metadata":{"id":"cas5F8paD1WV","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io\n","import math\n","import sklearn\n","import sklearn.datasets\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8upBBDMZJnGN","colab_type":"text"},"source":["# Adam"]},{"cell_type":"markdown","metadata":{"id":"Fffy7rLxJpsu","colab_type":"text"},"source":["## 4 - Adam\n","\n","Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n","\n","**How does Adam work?**\n","1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n","2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n","3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n","\n","The update rule is, for $l = 1, ..., L$: \n","\n","$$\\begin{cases}\n","v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n","v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n","s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n","s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n","W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n","\\end{cases}$$\n","where:\n","- t counts the number of steps taken of Adam \n","- L is the number of layers\n","- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n","- $\\alpha$ is the learning rate\n","- $\\varepsilon$ is a very small number to avoid dividing by zero\n","\n","As usual, we will store all parameters in the `parameters` dictionary  "]},{"cell_type":"markdown","metadata":{"id":"lq_QpR4WKzWe","colab_type":"text"},"source":["**Exercise**: Initialize the Adam variables $v, s$ which keep track of the past information.\n","\n","**Instruction**: The variables $v, s$ are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for `grads`, that is:\n","for $l = 1, ..., L$:\n","```python\n","v[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\n","v[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n","s[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\n","s[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n","\n","```"]},{"cell_type":"code","metadata":{"id":"agDV5-C5KKX9","colab_type":"code","colab":{}},"source":["def initialize_adam(parameters) :\n","    \"\"\"\n","    Initializes v and s as two python dictionaries with:\n","                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n","                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters.\n","                    parameters[\"W\" + str(l)] = Wl\n","                    parameters[\"b\" + str(l)] = bl\n","    \n","    Returns: \n","    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n","                    v[\"dW\" + str(l)] = ...\n","                    v[\"db\" + str(l)] = ...\n","    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n","                    s[\"dW\" + str(l)] = ...\n","                    s[\"db\" + str(l)] = ...\n","\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural networks\n","    v = {}\n","    s = {}\n","    \n","    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n","    for l in range(L):\n","    ### START CODE HERE ### (approx. 4 lines)\n","        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n","        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n","\n","        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n","        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n","    ### END CODE HERE ###\n","    \n","    return v, s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpsSnGZEK22F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"1d96ba8e-3292-485c-8c1e-e7065df1c3e4","executionInfo":{"status":"ok","timestamp":1591003028250,"user_tz":-120,"elapsed":569,"user":{"displayName":"Nicol√≤ Marana","photoUrl":"","userId":"03353917979045990121"}}},"source":["parameters = initialize_adam_test_case()\n","\n","v, s = initialize_adam(parameters)\n","print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n","print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n","print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n","print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n","print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n","print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n","print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n","print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["v[\"dW1\"] = [[0. 0. 0.]\n"," [0. 0. 0.]]\n","v[\"db1\"] = [[0.]\n"," [0.]]\n","v[\"dW2\"] = [[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","v[\"db2\"] = [[0.]\n"," [0.]\n"," [0.]]\n","s[\"dW1\"] = [[0. 0. 0.]\n"," [0. 0. 0.]]\n","s[\"db1\"] = [[0.]\n"," [0.]]\n","s[\"dW2\"] = [[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","s[\"db2\"] = [[0.]\n"," [0.]\n"," [0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hyeoorg5K-Ht","colab_type":"text"},"source":["**Exercise**:  Now, implement the parameters update with Adam. Recall the general update rule is, for $l = 1, ..., L$: \n","\n","$$\\begin{cases}\n","v_{W^{[l]}} = \\beta_1 v_{W^{[l]}} + (1 - \\beta_1) \\frac{\\partial J }{ \\partial W^{[l]} } \\\\\n","v^{corrected}_{W^{[l]}} = \\frac{v_{W^{[l]}}}{1 - (\\beta_1)^t} \\\\\n","s_{W^{[l]}} = \\beta_2 s_{W^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial W^{[l]} })^2 \\\\\n","s^{corrected}_{W^{[l]}} = \\frac{s_{W^{[l]}}}{1 - (\\beta_2)^t} \\\\\n","W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{W^{[l]}}}{\\sqrt{s^{corrected}_{W^{[l]}}}+\\varepsilon}\n","\\end{cases}$$\n","\n","\n","**Note** that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift `l` to `l+1` when coding."]},{"cell_type":"code","metadata":{"id":"mEGvUzmTK5iA","colab_type":"code","colab":{}},"source":["def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n","                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n","    \"\"\"\n","    Update parameters using Adam\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters:\n","                    parameters['W' + str(l)] = Wl\n","                    parameters['b' + str(l)] = bl\n","    grads -- python dictionary containing your gradients for each parameters:\n","                    grads['dW' + str(l)] = dWl\n","                    grads['db' + str(l)] = dbl\n","    v -- Adam variable, moving average of the first gradient, python dictionary\n","    s -- Adam variable, moving average of the squared gradient, python dictionary\n","    learning_rate -- the learning rate, scalar.\n","    beta1 -- Exponential decay hyperparameter for the first moment estimates \n","    beta2 -- Exponential decay hyperparameter for the second moment estimates \n","    epsilon -- hyperparameter preventing division by zero in Adam updates\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","    v -- Adam variable, moving average of the first gradient, python dictionary\n","    s -- Adam variable, moving average of the squared gradient, python dictionary\n","    \"\"\"\n","    \n","    L = len(parameters) // 2                 # number of layers in the neural networks\n","    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n","    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n","    \n","    # Perform Adam update on all parameters\n","    for l in range(L):\n","        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n","        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n","        ### END CODE HERE ###\n","\n","        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n","        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n","        ### END CODE HERE ###\n","\n","        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n","        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n","        ### END CODE HERE ###\n","\n","        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n","        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n","        ### END CODE HERE ###\n","\n","        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n","        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n","        ### END CODE HERE ###\n","\n","    return parameters, v, s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyqXHtEwK8-l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":535},"outputId":"89f82943-51ee-486f-fa41-c0d6e97aa971","executionInfo":{"status":"ok","timestamp":1591003080543,"user_tz":-120,"elapsed":587,"user":{"displayName":"Nicol√≤ Marana","photoUrl":"","userId":"03353917979045990121"}}},"source":["parameters, grads, v, s = update_parameters_with_adam_test_case()\n","parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))\n","print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n","print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n","print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n","print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n","print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n","print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n","print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n","print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["W1 = [[ 1.63178673 -0.61919778 -0.53561312]\n"," [-1.08040999  0.85796626 -2.29409733]]\n","b1 = [[ 1.75225313]\n"," [-0.75376553]]\n","W2 = [[ 0.32648046 -0.25681174  1.46954931]\n"," [-2.05269934 -0.31497584 -0.37661299]\n"," [ 1.14121081 -1.09245036 -0.16498684]]\n","b2 = [[-0.88529978]\n"," [ 0.03477238]\n"," [ 0.57537385]]\n","v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n"," [ 0.05024943  0.09008559 -0.06837279]]\n","v[\"db1\"] = [[-0.01228902]\n"," [-0.09357694]]\n","v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n"," [-0.03967535 -0.06871727 -0.08452056]\n"," [-0.06712461 -0.00126646 -0.11173103]]\n","v[\"db2\"] = [[0.02344157]\n"," [0.16598022]\n"," [0.07420442]]\n","s[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n"," [0.0002525  0.00081154 0.00046748]]\n","s[\"db1\"] = [[1.51020075e-05]\n"," [8.75664434e-04]]\n","s[\"dW2\"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n"," [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n"," [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\n","s[\"db2\"] = [[5.49507194e-05]\n"," [2.75494327e-03]\n"," [5.50629536e-04]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fo7EGlL8LGS6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}